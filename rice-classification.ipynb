{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# <b>1 <span style='color:#78D118'>|</span> Introduction</b>\n![](http://www.isaaa.org/kc/cropbiotechupdate/files/images/9252019125259AM.jpg)\n\n\nIn this notebook I'm gonna be using Transfer Learning MobileNetv2 by Keras to make a classification model for our dataset.\n\n### Dataset Overview\nThis dataset includes 5 different rice types images with 15000 images for every category. And our task is to make a classification model that could correctly predict the 5 kinds of rice.\n\n#### Rice Types\n* Arborio\n* Basmati\n* Ipsala\n* Jasmine\n* Karacadag","metadata":{}},{"cell_type":"markdown","source":"# <b>2 <span style='color:#78D118'>|</span> Preparing the Data</b>","metadata":{}},{"cell_type":"code","source":"# Importing necessary libraries\n\n\nimport tensorflow as tf \nfrom tensorflow import keras \n\nimport tensorflow_hub as hub \n\nfrom sklearn.model_selection import train_test_split\n\n\nimport matplotlib.pyplot as plt\nimport matplotlib.image as img\nimport PIL.Image as Image\nimport cv2\n\nimport os\nimport numpy as np\nimport pathlib","metadata":{"execution":{"iopub.status.busy":"2022-05-23T06:57:43.687743Z","iopub.execute_input":"2022-05-23T06:57:43.688044Z","iopub.status.idle":"2022-05-23T06:57:52.169370Z","shell.execute_reply.started":"2022-05-23T06:57:43.688015Z","shell.execute_reply":"2022-05-23T06:57:52.168596Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"**Preparing our dataset**","metadata":{}},{"cell_type":"code","source":"data_dir = \"../input/rice-image-dataset/Rice_Image_Dataset\" # Datasets path\ndata_dir = pathlib.Path(data_dir)\ndata_dir","metadata":{"execution":{"iopub.status.busy":"2022-05-23T06:57:52.170757Z","iopub.execute_input":"2022-05-23T06:57:52.170983Z","iopub.status.idle":"2022-05-23T06:57:52.179140Z","shell.execute_reply.started":"2022-05-23T06:57:52.170956Z","shell.execute_reply":"2022-05-23T06:57:52.178319Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"**Separating the categories**","metadata":{}},{"cell_type":"code","source":"arborio = list(data_dir.glob('Arborio/*'))[:600]\nbasmati = list(data_dir.glob('Basmati/*'))[:600]\nipsala = list(data_dir.glob('Ipsala/*'))[:600]\njasmine = list(data_dir.glob('Jasmine/*'))[:600]\nkaracadag = list(data_dir.glob('Karacadag/*'))[:600]","metadata":{"execution":{"iopub.status.busy":"2022-05-23T06:57:52.180470Z","iopub.execute_input":"2022-05-23T06:57:52.181197Z","iopub.status.idle":"2022-05-23T06:57:56.994837Z","shell.execute_reply.started":"2022-05-23T06:57:52.181154Z","shell.execute_reply":"2022-05-23T06:57:56.993396Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"**Checking samples**","metadata":{}},{"cell_type":"code","source":"arborio[:2]","metadata":{"execution":{"iopub.status.busy":"2022-05-23T06:57:56.996922Z","iopub.execute_input":"2022-05-23T06:57:56.997176Z","iopub.status.idle":"2022-05-23T06:57:57.003942Z","shell.execute_reply.started":"2022-05-23T06:57:56.997148Z","shell.execute_reply":"2022-05-23T06:57:57.002771Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"fig, ax = plt.subplots(ncols=5, figsize=(20,5))\nfig.suptitle('Rice Category')\narborio_image = img.imread(arborio[0])\nbasmati_image = img.imread(basmati[0])\nipsala_image = img.imread(ipsala[0])\njasmine_image = img.imread(jasmine[0])\nkaracadag_image = img.imread(karacadag[0])\n\nax[0].set_title('arborio')\nax[1].set_title('basmati')\nax[2].set_title('ipsala')\nax[3].set_title('jasmine')\nax[4].set_title('karacadag')\n\n\nax[0].imshow(arborio_image)\nax[1].imshow(basmati_image)\nax[2].imshow(ipsala_image)\nax[3].imshow(jasmine_image)\nax[4].imshow(karacadag_image)","metadata":{"execution":{"iopub.status.busy":"2022-05-23T06:57:57.005700Z","iopub.execute_input":"2022-05-23T06:57:57.006045Z","iopub.status.idle":"2022-05-23T06:57:57.839434Z","shell.execute_reply.started":"2022-05-23T06:57:57.005999Z","shell.execute_reply":"2022-05-23T06:57:57.838178Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"**Assigning a separate dictionary for images and their corresponding labels**","metadata":{}},{"cell_type":"code","source":"# Contains the images path\ndf_images = {\n    'arborio' : arborio,\n    'basmati' : basmati,\n    'ipsala' : ipsala,\n    'jasmine' : jasmine,\n    'karacadag': karacadag\n}\n\n# Contains numerical labels for the categories\ndf_labels = {\n    'arborio' : 0,\n    'basmati' : 1,\n    'ipsala' : 2,\n    'jasmine' : 3,\n    'karacadag': 4\n}","metadata":{"execution":{"iopub.status.busy":"2022-05-23T06:57:57.841049Z","iopub.execute_input":"2022-05-23T06:57:57.841405Z","iopub.status.idle":"2022-05-23T06:57:57.848835Z","shell.execute_reply.started":"2022-05-23T06:57:57.841360Z","shell.execute_reply":"2022-05-23T06:57:57.847504Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":"**Since the MobileNetv2 training images dimensions are 224 by 224 by 3, we have to reshape our categories into that**","metadata":{}},{"cell_type":"code","source":"img = cv2.imread(str(df_images['arborio'][0])) # Converting it into numerical arrays\nimg.shape # Its currently 250 by 250 by 3","metadata":{"execution":{"iopub.status.busy":"2022-05-23T06:57:57.850875Z","iopub.execute_input":"2022-05-23T06:57:57.852271Z","iopub.status.idle":"2022-05-23T06:57:57.888266Z","shell.execute_reply.started":"2022-05-23T06:57:57.852217Z","shell.execute_reply":"2022-05-23T06:57:57.887268Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"X, y = [], [] # X = images, y = labels\nfor label, images in df_images.items():\n    for image in images:\n        img = cv2.imread(str(image))\n        resized_img = cv2.resize(img, (224, 224)) # Resizing the images to be able to pass on MobileNetv2 model\n        X.append(resized_img) \n        y.append(df_labels[label])","metadata":{"execution":{"iopub.status.busy":"2022-05-23T06:57:57.889254Z","iopub.execute_input":"2022-05-23T06:57:57.890255Z","iopub.status.idle":"2022-05-23T06:58:20.498882Z","shell.execute_reply.started":"2022-05-23T06:57:57.890213Z","shell.execute_reply":"2022-05-23T06:58:20.498305Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":"**Splitting the data and standarization**","metadata":{}},{"cell_type":"code","source":"# Standarizing\nX = np.array(X)\nX = X/255\ny = np.array(y)","metadata":{"execution":{"iopub.status.busy":"2022-05-23T06:58:20.499819Z","iopub.execute_input":"2022-05-23T06:58:20.500338Z","iopub.status.idle":"2022-05-23T06:58:22.332422Z","shell.execute_reply.started":"2022-05-23T06:58:20.500284Z","shell.execute_reply":"2022-05-23T06:58:22.331766Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"# Separating data into training, test and validation sets\nX_train, X_test_val, y_train, y_test_val = train_test_split(X, y)\nX_test, X_val, y_test, y_val = train_test_split(X_test_val, y_test_val)","metadata":{"execution":{"iopub.status.busy":"2022-05-23T06:58:22.334406Z","iopub.execute_input":"2022-05-23T06:58:22.334644Z","iopub.status.idle":"2022-05-23T06:58:24.051272Z","shell.execute_reply.started":"2022-05-23T06:58:22.334615Z","shell.execute_reply":"2022-05-23T06:58:24.050504Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":"# <b>3 <span style='color:#78D118'>|</span> Creating the Model</b>","metadata":{}},{"cell_type":"code","source":"mobile_net = 'https://tfhub.dev/google/tf2-preview/mobilenet_v2/feature_vector/4' # MobileNetv4 link\nmobile_net = hub.KerasLayer(\n        mobile_net, input_shape=(224,224, 3), trainable=False) # Removing the last layer","metadata":{"execution":{"iopub.status.busy":"2022-05-23T06:58:24.052518Z","iopub.execute_input":"2022-05-23T06:58:24.052728Z","iopub.status.idle":"2022-05-23T06:58:44.409362Z","shell.execute_reply.started":"2022-05-23T06:58:24.052702Z","shell.execute_reply":"2022-05-23T06:58:44.408013Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"num_label = 5 # number of labels\n\nmodel = keras.Sequential([\n    mobile_net,\n    keras.layers.Dense(num_label)\n])\n\nmodel.summary()","metadata":{"execution":{"iopub.status.busy":"2022-05-23T07:02:31.082963Z","iopub.execute_input":"2022-05-23T07:02:31.083266Z","iopub.status.idle":"2022-05-23T07:02:31.169475Z","shell.execute_reply.started":"2022-05-23T07:02:31.083235Z","shell.execute_reply":"2022-05-23T07:02:31.168218Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"markdown","source":"# <b>4 <span style='color:#78D118'>|</span> Training the Model</b>","metadata":{}},{"cell_type":"code","source":"model.compile(\n  optimizer=\"adam\",\n  loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n  metrics=['acc'])\n\n\nhistory = model.fit(X_train, y_train, epochs=10, validation_data=(X_val, y_val))","metadata":{"execution":{"iopub.status.busy":"2022-05-23T07:03:15.930092Z","iopub.execute_input":"2022-05-23T07:03:15.930926Z","iopub.status.idle":"2022-05-23T07:03:15.958219Z","shell.execute_reply.started":"2022-05-23T07:03:15.930881Z","shell.execute_reply":"2022-05-23T07:03:15.956977Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"markdown","source":"# <b>5 <span style='color:#78D118'>|</span> Evaluate the Model</b>\n\n#### I've evaluated the model using accuracy, recall, precision and f1-score","metadata":{}},{"cell_type":"code","source":"model.evaluate(X_test,y_test)","metadata":{"execution":{"iopub.status.busy":"2022-05-23T07:03:22.587892Z","iopub.execute_input":"2022-05-23T07:03:22.589446Z","iopub.status.idle":"2022-05-23T07:03:22.620279Z","shell.execute_reply.started":"2022-05-23T07:03:22.589380Z","shell.execute_reply":"2022-05-23T07:03:22.618384Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import classification_report\n\ny_pred = model.predict(X_test, batch_size=64, verbose=1)\ny_pred_bool = np.argmax(y_pred, axis=1)\n\nprint(classification_report(y_test, y_pred_bool))","metadata":{"execution":{"iopub.status.busy":"2022-05-23T06:58:44.415056Z","iopub.status.idle":"2022-05-23T06:58:44.415403Z","shell.execute_reply.started":"2022-05-23T06:58:44.415216Z","shell.execute_reply":"2022-05-23T06:58:44.415231Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# <b>6 <span style='color:#78D118'>|</span> Visualizing the Model</b>\n#### On how the models accuracy and loss changed through-out the 5 epochs","metadata":{}},{"cell_type":"code","source":"plt.plot(history.history['acc'], marker='o')\nplt.plot(history.history['val_acc'], marker='o')\nplt.title('model accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['train', 'val'], loc='lower right')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-05-23T06:58:44.416517Z","iopub.status.idle":"2022-05-23T06:58:44.416829Z","shell.execute_reply.started":"2022-05-23T06:58:44.416672Z","shell.execute_reply":"2022-05-23T06:58:44.416687Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.plot(history.history['loss'], marker='o')\nplt.plot(history.history['val_loss'], marker='o')\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'val'], loc='upper right')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-05-23T06:58:44.417840Z","iopub.status.idle":"2022-05-23T06:58:44.418151Z","shell.execute_reply.started":"2022-05-23T06:58:44.418001Z","shell.execute_reply":"2022-05-23T06:58:44.418017Z"},"trusted":true},"execution_count":null,"outputs":[]}]}